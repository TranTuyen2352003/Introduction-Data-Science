{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Data Science\n",
    "Programming Exercise: 04\n",
    "Name: Tran Thi Bich Tuyen\n",
    "Student ID: 21280059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tải về nội dung HTML từ một đường dẫn URL đã cho.\n",
    "def download_html(url):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        html = html.decode('utf-8')\n",
    "    response.close()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_links(url):\n",
    "    html = download_html(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    paper_links = []\n",
    "    for link in soup.find_all('a', {'title': 'Abstract'}):\n",
    "        paper_links.append('https://arxiv.org' + link['href'])\n",
    "\n",
    "    return paper_links\n",
    "\n",
    "url = 'https://arxiv.org/list/cs.AI/recent'\n",
    "paper_links = get_paper_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/abs/2305.11137\n",
      "https://arxiv.org/abs/2305.11130\n",
      "https://arxiv.org/abs/2305.11098\n",
      "https://arxiv.org/abs/2305.11087\n",
      "https://arxiv.org/abs/2305.11074\n",
      "https://arxiv.org/abs/2305.11014\n",
      "https://arxiv.org/abs/2305.10961\n",
      "https://arxiv.org/abs/2305.10938\n",
      "https://arxiv.org/abs/2305.10912\n",
      "https://arxiv.org/abs/2305.10883\n",
      "https://arxiv.org/abs/2305.10846\n",
      "https://arxiv.org/abs/2305.10834\n",
      "https://arxiv.org/abs/2305.10830\n",
      "https://arxiv.org/abs/2305.10783\n",
      "https://arxiv.org/abs/2305.10782\n",
      "https://arxiv.org/abs/2305.10766\n",
      "https://arxiv.org/abs/2305.10726\n",
      "https://arxiv.org/abs/2305.10708\n",
      "https://arxiv.org/abs/2305.10679\n",
      "https://arxiv.org/abs/2305.10654\n",
      "https://arxiv.org/abs/2305.10646\n",
      "https://arxiv.org/abs/2305.10563\n",
      "https://arxiv.org/abs/2305.10556\n",
      "https://arxiv.org/abs/2305.10538\n",
      "https://arxiv.org/abs/2305.10528\n"
     ]
    }
   ],
   "source": [
    "# In danh sách các đường dẫn đến các bài báo\n",
    "for link in paper_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_info(paper_url):\n",
    "    html = download_html(paper_url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Lấy tên bài báo\n",
    "    title = soup.find('h1', class_='title mathjax').text.strip()\n",
    "\n",
    "    # Lấy tên tác giả\n",
    "    authors = [author.text.strip() for author in soup.find_all('div', class_='authors')[0].find_all('a')]\n",
    "\n",
    "    # Lấy abstract của bài báo\n",
    "    abstract = soup.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "\n",
    "    # Lấy subjects của bài báo\n",
    "    subjects = [subject.text.strip() for subject in soup.find_all('span', class_='primary-subject')]\n",
    "\n",
    "    # Lấy đường dẫn download bài báo\n",
    "    download_url = None\n",
    "    download_link = soup.find('div', class_='full-text').find('a')\n",
    "    if download_link is not None:\n",
    "        download_url = download_link.get('href')\n",
    "        \n",
    "    # Trả về thông tin đã trích xuất\n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Authors': authors,\n",
    "        'Abstract': abstract,\n",
    "        'Subjects': subjects,\n",
    "        'DownloadUrl': download_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Title:Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion', 'Authors': ['Ashutosh Dutta', 'Milan Jain', 'Arif Khan', 'Arun Sathanur'], 'Abstract': 'Abstract:  Collisions, crashes, and other incidents on road networks, if left\\nunmitigated, can potentially cause cascading failures that can affect large\\nparts of the system. Timely handling such extreme congestion scenarios is\\nimperative to reduce emissions, enhance productivity, and improve the quality\\nof urban living. In this work, we propose a Deep Reinforcement Learning (DRL)\\napproach to reduce traffic congestion on multi-lane freeways during extreme\\ncongestion. The agent is trained to learn adaptive detouring strategies for\\ncongested freeway traffic such that the freeway lanes along with the local\\narterial network in proximity are utilized optimally, with rewards being\\ncongestion reduction and traffic speed improvement. The experimental setup is a\\n2.6-mile-long 4-lane freeway stretch in Shoreline, Washington, USA with two\\nexits and associated arterial roads simulated on a microscopic and continuous\\nmulti-modal traffic simulator SUMO (Simulation of Urban MObility) while using\\nparameterized traffic profiles generated using real-world traffic data. Our\\nanalysis indicates that DRL-based controllers can improve average traffic speed\\nby 21\\\\% when compared to no-action during steep congestion. The study further\\ndiscusses the trade-offs involved in the choice of reward functions, the impact\\nof human compliance on agent performance, and the feasibility of knowledge\\ntransfer from one agent to other to address data sparsity and scaling issues.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': \"Title:What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem\", 'Authors': ['Jose Alvarez Colmenares'], 'Abstract': 'Abstract:  The field of Artificial Intelligence (AI) is focusing on creating automated\\ndecision-making (ADM) systems that operate as close as possible to human-like\\nintelligence. This effort has pushed AI researchers into exploring cognitive\\nfields like psychology. The work of Daniel Kahneman and the late Amos Tversky\\non biased human decision-making, including the study of the conjunction\\nfallacy, has experienced a second revival because of this. Under the\\nconjunction fallacy a human decision-maker will go against basic probability\\nlaws and rank as more likely a conjunction over one of its parts. It has been\\nproven overtime through a set of experiments with the Linda Problem being the\\nmost famous one. Although this interdisciplinary effort is welcomed, we fear\\nthat AI researchers ignore the driving force behind the conjunction fallacy as\\ncaptured by the Linda Problem: the fact that Linda must be stereotypically\\ndescribed as a woman. In this paper we revisit the Linda Problem and formulate\\nit as a fairness problem. In doing so we introduce perception as a parameter of\\ninterest through the structural causal perception framework. Using an\\nillustrative decision-making example, we showcase the proposed conceptual\\nframework and its potential impact for developing fair ADM systems.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:The Hardness of Reasoning about Probabilities and Causality', 'Authors': ['Benito van der Zander', 'Markus Bläser', 'Maciej Liśkiewicz'], 'Abstract': 'Abstract:  We study formal languages which are capable of fully expressing quantitative\\nprobabilistic reasoning and do-calculus reasoning for causal effects, from a\\ncomputational complexity perspective. We focus on satisfiability problems whose\\ninstance formulas allow expressing many tasks in probabilistic and causal\\ninference. The main contribution of this work is establishing the exact\\ncomputational complexity of these satisfiability problems. We introduce a new\\nnatural complexity class, named succ$\\\\exists$R, which can be viewed as a\\nsuccinct variant of the well-studied class $\\\\exists$R, and show that the\\nproblems we consider are complete for succ$\\\\exists$R. Our results imply even\\nstronger algorithmic limitations than were proven by Fagin, Halpern, and\\nMegiddo (1990) and Mossé, Ibeling, and Icard (2022) for some variants of\\nthe standard languages used commonly in probabilistic and causal inference.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Efficient Computation of General Modules for ALC Ontologies (Extended Version)', 'Authors': ['Hui Yang', 'Patrick Koopmann', 'Yue Ma', 'Nicole Bidoit'], 'Abstract': 'Abstract:  We present a method for extracting general modules for ontologies formulated\\nin the description logic ALC. A module for an ontology is an ideally\\nsubstantially smaller ontology that preserves all entailments for a\\nuser-specified set of terms. As such, it has applications such as ontology\\nreuse and ontology analysis. Different from classical modules, general modules\\nmay use axioms not explicitly present in the input ontology, which allows for\\nadditional conciseness. So far, general modules have only been investigated for\\nlightweight description logics. We present the first work that considers the\\nmore expressive description logic ALC. In particular, our contribution is a new\\nmethod based on uniform interpolation supported by some new theoretical\\nresults. Our evaluation indicates that our general modules are often smaller\\nthan classical modules and uniform interpolants computed by the\\nstate-of-the-art, and compared with uniform interpolants, can be computed in a\\nsignificantly shorter time. Moreover, our method can be used for, and in fact\\nimproves, the computation of uniform interpolants and classical modules.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies', 'Authors': ['Uwe Peters', 'Mary Carman'], 'Abstract': 'Abstract:  Many ethical frameworks require artificial intelligence (AI) systems to be\\nexplainable. Explainable AI (XAI) models are frequently tested for their\\nadequacy in user studies. Since different people may have different explanatory\\nneeds, it is important that participant samples in user studies are large\\nenough to represent the target population to enable generalizations. However,\\nit is unclear to what extent XAI researchers reflect on and justify their\\nsample sizes or avoid broad generalizations across people. We analyzed XAI user\\nstudies (N = 220) published between 2012 and 2022. Most studies did not offer\\nrationales for their sample sizes. Moreover, most papers generalized their\\nconclusions beyond their target population, and there was no evidence that\\nbroader conclusions in quantitative studies were correlated with larger\\nsamples. These methodological problems can impede evaluations of whether XAI\\nsystems implement the explainability called for in ethical frameworks. We\\noutline principles for more inclusive XAI user studies.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Growing and Serving Large Open-domain Knowledge Graphs', 'Authors': ['Ihab F. Ilyas', 'JP Lacerda', 'Yunyao Li', 'Umar Farooq Minhas', 'Ali Mousavi', 'Jeffrey Pound', 'Theodoros Rekatsinas', 'Chiraag Sumanth'], 'Abstract': 'Abstract:  Applications of large open-domain knowledge graphs (KGs) to real-world\\nproblems pose many unique challenges. In this paper, we present extensions to\\nSaga our platform for continuous construction and serving of knowledge at\\nscale. In particular, we describe a pipeline for training knowledge graph\\nembeddings that powers key capabilities such as fact ranking, fact\\nverification, a related entities service, and support for entity linking. We\\nthen describe how our platform, including graph embeddings, can be leveraged to\\ncreate a Semantic Annotation service that links unstructured Web documents to\\nentities in our KG. Semantic annotation of the Web effectively expands our\\nknowledge graph with edges to open-domain Web content which can be used in\\nvarious search and ranking problems. Finally, we leverage annotated Web\\ndocuments to drive Open-domain Knowledge Extraction. This targeted extraction\\nframework identifies important coverage issues in the KG, then finds relevant\\ndata sources for target entities on the Web and extracts missing information to\\nenrich the KG. Finally, we describe adaptations to our knowledge platform\\nneeded to construct and serve private personal knowledge on-device. This\\nincludes private incremental KG construction, cross-device knowledge sync, and\\nglobal knowledge enrichment.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:A sequential transit network design algorithm with optimal learning under correlated beliefs', 'Authors': ['Gyugeun Yoon', 'Joseph Y. J. Chow'], 'Abstract': 'Abstract:  Mobility service route design requires potential demand information to well\\naccommodate travel demand within the service region. Transit planners and\\noperators can access various data sources including household travel survey\\ndata and mobile device location logs. However, when implementing a mobility\\nsystem with emerging technologies, estimating demand level becomes harder\\nbecause of more uncertainties with user behaviors. Therefore, this study\\nproposes an artificial intelligence-driven algorithm that combines sequential\\ntransit network design with optimal learning. An operator gradually expands its\\nroute system to avoid risks from inconsistency between designed routes and\\nactual travel demand. At the same time, observed information is archived to\\nupdate the knowledge that the operator currently uses. Three learning policies\\nare compared within the algorithm: multi-armed bandit, knowledge gradient, and\\nknowledge gradient with correlated beliefs. For validation, a new route system\\nis designed on an artificial network based on public use microdata areas in New\\nYork City. Prior knowledge is reproduced from the regional household travel\\nsurvey data. The results suggest that exploration considering correlations can\\nachieve better performance compared to greedy choices in general. In future\\nwork, the problem may incorporate more complexities such as demand elasticity\\nto travel time, no limitations to the number of transfers, and costs for\\nexpansion.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Establishing Shared Query Understanding in an Open Multi-Agent System', 'Authors': ['Nikolaos Kondylidis', 'Ilaria Tiddi', 'Annette ten Teije'], 'Abstract': 'Abstract:  We propose a method that allows to develop shared understanding between two\\nagents for the purpose of performing a task that requires cooperation. Our\\nmethod focuses on efficiently establishing successful task-oriented\\ncommunication in an open multi-agent system, where the agents do not know\\nanything about each other and can only communicate via grounded interaction.\\nThe method aims to assist researchers that work on human-machine interaction or\\nscenarios that require a human-in-the-loop, by defining interaction\\nrestrictions and efficiency metrics. To that end, we point out the challenges\\nand limitations of such a (diverse) setup, while also restrictions and\\nrequirements which aim to ensure that high task performance truthfully reflects\\nthe extent to which the agents correctly understand each other. Furthermore, we\\ndemonstrate a use-case where our method can be applied for the task of\\ncooperative query answering. We design the experiments by modifying an\\nestablished ontology alignment benchmark. In this example, the agents want to\\nquery each other, while representing different databases, defined in their own\\nontologies that contain different and incomplete knowledge. Grounded\\ninteraction here has the form of examples that consists of common instances,\\nfor which the agents are expected to have similar knowledge. Our experiments\\ndemonstrate successful communication establishment under the required\\nrestrictions, and compare different agent policies that aim to solve the task\\nin an efficient manner.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Rounding Meets Approximate Model Counting', 'Authors': ['Jiong Yang', 'Kuldeep S. Meel'], 'Abstract': 'Abstract:  The problem of model counting, also known as #SAT, is to compute the number\\nof models or satisfying assignments of a given Boolean formula $F$. Model\\ncounting is a fundamental problem in computer science with a wide range of\\napplications. In recent years, there has been a growing interest in using\\nhashing-based techniques for approximate model counting that provide\\n$(\\\\varepsilon, \\\\delta)$-guarantees: i.e., the count returned is within a\\n$(1+\\\\varepsilon)$-factor of the exact count with confidence at least\\n$1-\\\\delta$. While hashing-based techniques attain reasonable scalability for\\nlarge enough values of $\\\\delta$, their scalability is severely impacted for\\nsmaller values of $\\\\delta$, thereby preventing their adoption in application\\ndomains that require estimates with high confidence.\\nThe primary contribution of this paper is to address the Achilles heel of\\nhashing-based techniques: we propose a novel approach based on rounding that\\nallows us to achieve a significant reduction in runtime for smaller values of\\n$\\\\delta$. The resulting counter, called RoundMC, achieves a substantial runtime\\nperformance improvement over the current state-of-the-art counter, ApproxMC. In\\nparticular, our extensive evaluation over a benchmark suite consisting of 1890\\ninstances shows that RoundMC solves 204 more instances than ApproxMC, and\\nachieves a $4\\\\times$ speedup over ApproxMC.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning', 'Authors': ['Hao Chen', 'Yiming Zhang', 'Qi Zhang', 'Hantao Yang', 'Xiaomeng Hu', 'Xuetao Ma', 'Yifan Yanggong', 'Junbo Zhao'], 'Abstract': 'Abstract:  Instruction tuning for large language models (LLMs) has gained attention from\\nresearchers due to its ability to unlock the potential of LLMs in following\\ninstructions. While instruction tuning offers advantages for facilitating the\\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\\napproach, training models with tens of millions or even billions of parameters\\non large amounts of data results in unaffordable computational costs. To\\naddress this, we focus on reducing the data used in LLM instruction tuning to\\ndecrease training costs and improve data efficiency, dubbed as Low Training\\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\\nconducts a preliminary exploration into reducing the data used in LLM training\\nand identifies several observations regarding task specialization for LLM\\ntraining, such as the optimization of performance for a specific task, the\\nnumber of instruction types required for instruction tuning, and the amount of\\ndata required for task-specific models. The results suggest that task-specific\\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\\nimprovement in performance over those trained on full task-related data.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Can we forget how we learned? Representing states in iterated belief revision}', 'Authors': ['Paolo Liberatore'], 'Abstract': 'Abstract:  The three most common representations of states in iterated belief revision\\nare compared: explicit, by levels and by history. The first is a connected\\npreorder between models, the second is a list of formulae representing\\nequivalence classes, the third is the sequence of the previous revisions. The\\nlatter depends on the revision semantics and on history rewriting, and the\\nlatter depends on the allowed rewritings. All mechanisms represent all possible\\nstates. A rewritten history of lexicographic revision is more efficient than\\nthe other considered representations in terms of size with arbitrary history\\nrewritings. Establishing the redundancy of such a history is a mild rewriting.\\nIt is coNP-complete in the general case, and is hard even on histories of two\\nrevisions or revisions of arbitrary length of Horn formulae, and is polynomial\\non histories of two Horn formulae. A minor technical result is a\\npolynomial-time algorithm for establishing whether a Horn formula is equivalent\\nto the negation of another Horn formula.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network for Adaptive Motion Integration', 'Authors': ['Zitang Sun', 'Yen-Ju Chen', 'Yung-hao Yang', \"Shin'ya Nishida\"], 'Abstract': \"Abstract:  Visual motion processing is essential for organisms to perceive and interact\\nwith dynamic environments. Despite extensive research in cognitive\\nneuroscience, image-computable models that can extract informative motion flow\\nfrom natural scenes in a manner consistent with human visual processing have\\nyet to be established. Meanwhile, recent advancements in computer vision (CV),\\npropelled by deep learning, have led to significant progress in optical flow\\nestimation, a task closely related to motion perception. Here we propose an\\nimage-computable model of human motion perception by bridging the gap between\\nhuman and CV models. Specifically, we introduce a novel two-stage approach that\\ncombines trainable motion energy sensing with a recurrent self-attention\\nnetwork for adaptive motion integration and segregation. This model\\narchitecture aims to capture the computations in V1-MT, the core structure for\\nmotion perception in the biological visual system. In silico neurophysiology\\nreveals that our model's unit responses are similar to mammalian neural\\nrecordings regarding motion pooling and speed tuning. The proposed model can\\nalso replicate human responses to a range of stimuli examined in past\\npsychophysical studies. The experimental results on the Sintel benchmark\\ndemonstrate that our model predicts human responses better than the ground\\ntruth, whereas the CV models show the opposite. Further partial correlation\\nanalysis indicates our model outperforms several state-of-the-art CV models in\\nexplaining the human responses that deviate from the ground truth. Our study\\nprovides a computational architecture consistent with human visual motion\\nprocessing, although the physiological correspondence may not be exact.\", 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:On Optimal Strategies for Wordle and General Guessing Games', 'Authors': ['Michael Cunanan', 'Michael Thielscher'], 'Abstract': 'Abstract:  The recent popularity of Wordle has revived interest in guessing games. We\\ndevelop a general method for finding optimal strategies for guessing games\\nwhile avoiding an exhaustive search. Our main contributions are several\\ntheorems that build towards a general theory to prove the optimality of a\\nstrategy for a guessing game. This work is developed to apply to any guessing\\ngame, but we use Wordle as an example to present concrete results.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition', 'Authors': ['Brendan Conway-Smith', 'Robert L. West'], 'Abstract': 'Abstract:  Attempts to import dual-system descriptions of System-1 and System-2 into AI\\nhave been hindered by a lack of clarity over their distinction. We address this\\nand other issues by situating System-1 and System-2 within the Common Model of\\nCognition. Results show that what are thought to be distinctive characteristics\\nof System-1 and 2 instead form a spectrum of cognitive properties. The Common\\nModel provides a comprehensive vision of the computational units involved in\\nSystem-1 and System-2, their underlying mechanisms, and the implications for\\nlearning, metacognition, and emotion.', 'Subjects': ['Artificial Intelligence (cs.AI)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation', 'Authors': ['Samaneh Azadi', 'Akbar Shah', 'Thomas Hayes', 'Devi Parikh', 'Sonal Gupta'], 'Abstract': 'Abstract:  Text-guided human motion generation has drawn significant interest because of\\nits impactful applications spanning animation and robotics. Recently,\\napplication of diffusion models for motion generation has enabled improvements\\nin the quality of generated motions. However, existing approaches are limited\\nby their reliance on relatively small-scale motion capture data, leading to\\npoor performance on more diverse, in-the-wild prompts. In this paper, we\\nintroduce Make-An-Animation, a text-conditioned human motion generation model\\nwhich learns more diverse poses and prompts from large-scale image-text\\ndatasets, enabling significant improvement in performance over prior works.\\nMake-An-Animation is trained in two stages. First, we train on a curated\\nlarge-scale dataset of (text, static pseudo-pose) pairs extracted from\\nimage-text datasets. Second, we fine-tune on motion capture data, adding\\nadditional layers to model the temporal dimension. Unlike prior diffusion\\nmodels for motion generation, Make-An-Animation uses a U-Net architecture\\nsimilar to recent text-to-video generation models. Human evaluation of motion\\nrealism and alignment with input text shows that our model reaches\\nstate-of-the-art performance on text-to-motion generation.', 'Subjects': ['Computer Vision and Pattern Recognition (cs.CV)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage', 'Authors': ['Jose Blanchet', 'Miao Lu', 'Tong Zhang', 'Han Zhong'], 'Abstract': 'Abstract:  We study distributionally robust offline reinforcement learning (robust\\noffline RL), which seeks to find an optimal robust policy purely from an\\noffline dataset that can perform well in perturbed environments. We propose a\\ngeneric algorithm framework \\\\underline{D}oubly \\\\underline{P}essimistic\\n\\\\underline{M}odel-based \\\\underline{P}olicy \\\\underline{O}ptimization\\n($\\\\texttt{P}^2\\\\texttt{MPO}$) for robust offline RL, which features a novel\\ncombination of a flexible model estimation subroutine and a doubly pessimistic\\npolicy optimization step. The \\\\emph{double pessimism} principle is crucial to\\novercome the distributional shift incurred by i) the mismatch between behavior\\npolicy and the family of target policies; and ii) the perturbation of the\\nnominal model. Under certain accuracy assumptions on the model estimation\\nsubroutine, we show that $\\\\texttt{P}^2\\\\texttt{MPO}$ is provably efficient with\\n\\\\emph{robust partial coverage data}, which means that the offline dataset has\\ngood coverage of the distributions induced by the optimal robust policy and\\nperturbed models around the nominal model. By tailoring specific model\\nestimation subroutines for concrete examples including tabular Robust Markov\\nDecision Process (RMDP), factored RMDP, and RMDP with kernel and neural\\nfunction approximations, we show that $\\\\texttt{P}^2\\\\texttt{MPO}$ enjoys a\\n$\\\\tilde{\\\\mathcal{O}}(n^{-1/2})$ convergence rate, where $n$ is the number of\\ntrajectories in the offline dataset. Notably, these models, except for the\\ntabular case, are first identified and proven tractable by this paper. To the\\nbest of our knowledge, we first propose a general learning principle -- double\\npessimism -- for robust offline RL and show that it is provably efficient in\\nthe context of general function approximations.', 'Subjects': ['Machine Learning (cs.LG)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Satisfiability-Aided Language Models Using Declarative Prompting', 'Authors': ['Xi Ye', 'Qiaochu Chen', 'Isil Dillig', 'Greg Durrett'], 'Abstract': 'Abstract:  Prior work has combined chain-of-thought prompting in large language models\\n(LLMs) with programmatic representations to perform effective and transparent\\nreasoning. While such an approach works very well for tasks that only require\\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\\nconstraint solving tasks that require more sophisticated planning and search.\\nIn this paper, we propose a new satisfiability-aided language modeling approach\\nfor improving the reasoning capabilities of LLMs. We use an LLM to generate a\\ndeclarative task specification rather than an imperative program and leverage\\nan off-the-shelf automated theorem prover to derive the final answer. This\\napproach has two key advantages. The declarative specification is closer to the\\nproblem description than the reasoning steps are, so the LLM can parse it more\\naccurately. Furthermore, by offloading the actual reasoning task to an\\nautomated theorem prover, our approach can guarantee the correctness of the\\nanswer with respect to the parsed specification and avoid planning errors in\\nthe reasoning process. We evaluate SATLM on 6 different datasets and show that\\nit consistently outperforms program-aided LMs in an imperative paradigm\\n(PROGLM). In particular, SATLM outperforms PROGLM by 23% on a challenging\\nsubset of GSM; SATLM also achieves a new SoTA on LSAT, surpassing previous\\nmodels that are trained on the full training set.', 'Subjects': ['Computation and Language (cs.CL)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Prompt-Tuning Decision Transformer with Preference Ranking', 'Authors': ['Shengchao Hu', 'Li Shen', 'Ya Zhang', 'Dacheng Tao'], 'Abstract': 'Abstract:  Prompt-tuning has emerged as a promising method for adapting pre-trained\\nmodels to downstream tasks or aligning with human preferences. Prompt learning\\nis widely used in NLP but has limited applicability to RL due to the complex\\nphysical meaning and environment-specific information contained within RL\\nprompts. These factors require supervised learning to imitate the\\ndemonstrations and may result in a loss of meaning after learning.\\nAdditionally, directly extending prompt-tuning approaches to RL is challenging\\nbecause RL prompts guide agent behavior based on environmental modeling and\\nanalysis, rather than filling in missing information, making it unlikely that\\nadjustments to the prompt format for downstream tasks, as in NLP, can yield\\nsignificant improvements. In this work, we propose the Prompt-Tuning DT\\nalgorithm to address these challenges by using trajectory segments as prompts\\nto guide RL agents in acquiring environmental information and optimizing\\nprompts via black-box tuning to enhance their ability to contain more relevant\\ninformation, thereby enabling agents to make better decisions. Our approach\\ninvolves randomly sampling a Gaussian distribution to fine-tune the elements of\\nthe prompt trajectory and using preference ranking function to find the\\noptimization direction, thereby providing more informative prompts and guiding\\nthe agent towards specific preferences in the target environment. Extensive\\nexperiments show that with only 0.03% of the parameters learned, Prompt-Tuning\\nDT achieves comparable or even better performance than full-model fine-tuning\\nin low-data scenarios. Our work contributes to the advancement of prompt-tuning\\napproaches in RL, providing a promising direction for optimizing large RL\\nagents for specific preference tasks.', 'Subjects': ['Machine Learning (cs.LG)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys', 'Authors': ['Junsol Kim', 'Byungkyu Lee'], 'Abstract': \"Abstract:  How can we use large language models (LLMs) to augment surveys? This paper\\ninvestigates three distinct applications of LLMs fine-tuned by nationally\\nrepresentative surveys for opinion prediction -- missing data imputation,\\nretrodiction, and zero-shot prediction. We present a new methodological\\nframework that incorporates neural embeddings of survey questions, individual\\nbeliefs, and temporal contexts to personalize LLMs in opinion prediction. Among\\n3,110 binarized opinions from 68,846 Americans in the General Social Survey\\nfrom 1972 to 2021, our best models based on Alpaca-7b excels in missing data\\nimputation (AUC = 0.87 for personal opinion prediction and $\\\\rho$ = 0.99 for\\npublic opinion prediction) and retrodiction (AUC = 0.86, $\\\\rho$ = 0.98). These\\nremarkable prediction capabilities allow us to fill in missing trends with high\\nconfidence and pinpoint when public attitudes changed, such as the rising\\nsupport for same-sex marriage. However, the models show limited performance in\\na zero-shot prediction task (AUC = 0.73, $\\\\rho$ = 0.67), highlighting\\nchallenges presented by LLMs without human responses. Further, we find that the\\nbest models' accuracy is lower for individuals with low socioeconomic status,\\nracial minorities, and non-partisan affiliations but higher for ideologically\\nsorted opinions in contemporary periods. We discuss practical constraints,\\nsocio-demographic representation, and ethical concerns regarding individual\\nautonomy and privacy when using LLMs for opinion prediction. This paper\\nshowcases a new approach for leveraging LLMs to enhance nationally\\nrepresentative surveys by predicting missing responses and trends.\", 'Subjects': ['Computation and Language (cs.CL)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Towards Expert-Level Medical Question Answering with Large Language Models', 'Authors': ['Karan Singhal', 'Tao Tu', 'Juraj Gottweis', 'Rory Sayres', 'Ellery Wulczyn', 'Le Hou', 'Kevin Clark', 'Stephen Pfohl', 'Heather Cole-Lewis', 'Darlene Neal', 'Mike Schaekermann', 'Amy Wang', 'Mohamed Amin', 'Sami Lachgar', 'Philip Mansfield', 'Sushant Prakash', 'Bradley Green', 'Ewa Dominowska', 'Blaise Aguera y Arcas', 'Nenad Tomasev', 'Yun Liu', 'Renee Wong', 'Christopher Semturs', 'S. Sara Mahdavi', 'Joelle Barral', 'Dale Webster', 'Greg S. Corrado', 'Yossi Matias', 'Shekoofeh Azizi', 'Alan Karthikesalingam', 'Vivek Natarajan'], 'Abstract': 'Abstract:  Recent artificial intelligence (AI) systems have reached milestones in \"grand\\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\\nmedical knowledge, reason over it, and answer medical questions comparably to\\nphysicians has long been viewed as one such grand challenge.\\nLarge language models (LLMs) have catalyzed significant progress in medical\\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\\non the MedQA dataset. However, this and other prior work suggested significant\\nroom for improvement, especially when models\\' answers were compared to\\nclinicians\\' answers. Here we present Med-PaLM 2, which bridges these gaps by\\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\\nfinetuning, and prompting strategies including a novel ensemble refinement\\napproach.\\nMed-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\\nby over 19% and setting a new state-of-the-art. We also observed performance\\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\\nclinical topics datasets.\\nWe performed detailed human evaluations on long-form questions along multiple\\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\\n< 0.001). We also observed significant improvements compared to Med-PaLM on\\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\\n\"adversarial\" questions to probe LLM limitations.\\nWhile further studies are necessary to validate the efficacy of these models\\nin real-world settings, these results highlight rapid progress towards\\nphysician-level performance in medical question answering.', 'Subjects': ['Computation and Language (cs.CL)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow', 'Authors': ['Denis Gudovskiy', 'Tomoyuki Okuno', 'Yohei Nakata'], 'Abstract': 'Abstract:  Recent semantic segmentation models accurately classify test-time examples\\nthat are similar to a training dataset distribution. However, their\\ndiscriminative closed-set approach is not robust in practical data setups with\\ndistributional shifts and out-of-distribution (OOD) classes. As a result, the\\npredicted probabilities can be very imprecise when used as confidence scores at\\ntest time. To address this, we propose a generative model for concurrent\\nin-distribution misclassification (IDM) and OOD detection that relies on a\\nnormalizing flow framework. The proposed flow-based detector with an\\nenergy-based inputs (FlowEneDet) can extend previously deployed segmentation\\nmodels without their time-consuming retraining. Our FlowEneDet results in a\\nlow-complexity architecture with marginal increase in the memory footprint.\\nFlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes\\nand SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to\\npretrained DeepLabV3+ and SegFormer semantic segmentation models.', 'Subjects': ['Computer Vision and Pattern Recognition (cs.CV)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Revisiting Proprioceptive Sensing for Articulated Object Manipulation', 'Authors': ['Thomas Lips', 'Francis wyffels'], 'Abstract': 'Abstract:  Robots that assist humans will need to interact with articulated objects such\\nas cabinets or microwaves. Early work on creating systems for doing so used\\nproprioceptive sensing to estimate joint mechanisms during contact. However,\\nnowadays, almost all systems use only vision and no longer consider\\nproprioceptive information during contact. We believe that proprioceptive\\ninformation during contact is a valuable source of information and did not find\\nclear motivation for not using it in the literature. Therefore, in this paper,\\nwe create a system that, starting from a given grasp, uses proprioceptive\\nsensing to open cabinets with a position-controlled robot and a parallel\\ngripper. We perform a qualitative evaluation of this system, where we find that\\nslip between the gripper and handle limits the performance. Nonetheless, we\\nfind that the system already performs quite well. This poses the question:\\nshould we make more use of proprioceptive information during contact in\\narticulated object manipulation systems, or is it not worth the added\\ncomplexity, and can we manage with vision alone? We do not have an answer to\\nthis question, but we hope to spark some discussion on the matter. The codebase\\nand videos of the system are available at\\nthis https URL.', 'Subjects': ['Robotics (cs.RO)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:UOR: Universal Backdoor Attacks on Pre-trained Language Models', 'Authors': ['Wei Du', 'Peixuan Li', 'Boqun Li', 'Haodong Zhao', 'Gongshen Liu'], 'Abstract': 'Abstract:  Backdoors implanted in pre-trained language models (PLMs) can be transferred\\nto various downstream tasks, which exposes a severe security threat. However,\\nmost existing backdoor attacks against PLMs are un-targeted and task-specific.\\nFew targeted and task-agnostic methods use manually pre-defined triggers and\\noutput representations, which prevent the attacks from being more effective and\\ngeneral. In this paper, we first summarize the requirements that a more\\nthreatening backdoor attack against PLMs should satisfy, and then propose a new\\nbackdoor attack method called UOR, which breaks the bottleneck of the previous\\napproach by turning manual selection into automatic optimization. Specifically,\\nwe define poisoned supervised contrastive learning which can automatically\\nlearn the more uniform and universal output representations of triggers for\\nvarious PLMs. Moreover, we use gradient search to select appropriate trigger\\nwords which can be adaptive to different PLMs and vocabularies. Experiments\\nshow that our method can achieve better attack performance on various text\\nclassification tasks compared to manual methods. Further, we tested our method\\non PLMs with different architectures, different usage paradigms, and more\\ndifficult tasks, which demonstrated the universality of our method.', 'Subjects': ['Computation and Language (cs.CL)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs', 'Authors': ['Sanna J. Ali', 'Angèle Christin', 'Andrew Smart', 'Riitta Katila'], 'Abstract': 'Abstract:  Amidst decline in public trust in technology, computing ethics have taken\\ncenter stage, and critics have raised questions about corporate ethics washing.\\nYet few studies examine the actual implementation of AI ethics values in\\ntechnology companies. Based on a qualitative analysis of technology workers\\ntasked with integrating AI ethics into product development, we find that\\nworkers experience an environment where policies, practices, and outcomes are\\ndecoupled. We analyze AI ethics workers as ethics entrepreneurs who work to\\ninstitutionalize new ethics-related practices within organizations. We show\\nthat ethics entrepreneurs face three major barriers to their work. First, they\\nstruggle to have ethics prioritized in an environment centered around software\\nproduct launches. Second, ethics are difficult to quantify in a context where\\ncompany goals are incentivized by metrics. Third, the frequent reorganization\\nof teams makes it difficult to access knowledge and maintain relationships\\ncentral to their work. Consequently, individuals take on great personal risk\\nwhen raising ethics issues, especially when they come from marginalized\\nbackgrounds. These findings shed light on complex dynamics of institutional\\nchange at technology companies.', 'Subjects': ['Computers and Society (cs.CY)'], 'DownloadUrl': None}\n",
      "{'Title': 'Title:Learning from Aggregated Data: Curated Bags versus Random Bags', 'Authors': ['Lin Chen', 'Thomas Fu', 'Amin Karbasi', 'Vahab Mirrokni'], 'Abstract': 'Abstract:  Protecting user privacy is a major concern for many machine learning systems\\nthat are deployed at scale and collect from a diverse set of population. One\\nway to address this concern is by collecting and releasing data labels in an\\naggregated manner so that the information about a single user is potentially\\ncombined with others. In this paper, we explore the possibility of training\\nmachine learning models with aggregated data labels, rather than individual\\nlabels. Specifically, we consider two natural aggregation procedures suggested\\nby practitioners: curated bags where the data points are grouped based on\\ncommon features and random bags where the data points are grouped randomly in\\nbag of similar sizes. For the curated bag setting and for a broad range of loss\\nfunctions, we show that we can perform gradient-based learning without any\\ndegradation in performance that may result from aggregating data. Our method is\\nbased on the observation that the sum of the gradients of the loss function on\\nindividual data examples in a curated bag can be computed from the aggregate\\nlabel without the need for individual labels. For the random bag setting, we\\nprovide a generalization risk bound based on the Rademacher complexity of the\\nhypothesis class and show how empirical risk minimization can be regularized to\\nachieve the smallest risk bound. In fact, in the random bag setting, there is a\\ntrade-off between size of the bag and the achievable error rate as our bound\\nindicates. Finally, we conduct a careful empirical study to confirm our\\ntheoretical findings. In particular, our results suggest that aggregate\\nlearning can be an effective method for preserving user privacy while\\nmaintaining model accuracy.', 'Subjects': ['Machine Learning (cs.LG)'], 'DownloadUrl': None}\n"
     ]
    }
   ],
   "source": [
    "# Duyệt qua danh sách đường dẫn và trích xuất thông tin từ mỗi bài báo\n",
    "for paper_link in paper_links:\n",
    "    paper_info = extract_paper_info(paper_link)\n",
    "    print(paper_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Tạo tệp CSV và ghi thông tin vào\n",
    "with open('paper_info.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Authors', 'Abstract', 'Subjects', 'DownloadUrl'])  # Ghi dòng tiêu đề\n",
    "\n",
    "    # Duyệt qua danh sách đường dẫn và trích xuất thông tin từ mỗi bài báo\n",
    "    for paper_link in paper_links:\n",
    "        paper_info = extract_paper_info(paper_link)\n",
    "        writer.writerow([paper_info['Title'], ', '.join(paper_info['Authors']), paper_info['Abstract'],\n",
    "                         ', '.join(paper_info['Subjects']), paper_info['DownloadUrl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  \\\n",
      "0   Title:Deep Reinforcement Learning to Maximize ...   \n",
      "1   Title:What's the Problem, Linda? The Conjuncti...   \n",
      "2   Title:The Hardness of Reasoning about Probabil...   \n",
      "3   Title:Efficient Computation of General Modules...   \n",
      "4   Title:Unjustified Sample Sizes and Generalizat...   \n",
      "5   Title:Growing and Serving Large Open-domain Kn...   \n",
      "6   Title:A sequential transit network design algo...   \n",
      "7   Title:Establishing Shared Query Understanding ...   \n",
      "8     Title:Rounding Meets Approximate Model Counting   \n",
      "9   Title:Maybe Only 0.5% Data is Needed: A Prelim...   \n",
      "10  Title:Can we forget how we learned? Representi...   \n",
      "11  Title:Modelling Human Visual Motion Processing...   \n",
      "12  Title:On Optimal Strategies for Wordle and Gen...   \n",
      "13  Title:AAAI 2022 Fall Symposium: System-1 and S...   \n",
      "14  Title:Make-An-Animation: Large-Scale Text-cond...   \n",
      "15  Title:Double Pessimism is Provably Efficient f...   \n",
      "16  Title:Satisfiability-Aided Language Models Usi...   \n",
      "17  Title:Prompt-Tuning Decision Transformer with ...   \n",
      "18  Title:AI-Augmented Surveys: Leveraging Large L...   \n",
      "19  Title:Towards Expert-Level Medical Question An...   \n",
      "20  Title:Concurrent Misclassification and Out-of-...   \n",
      "21  Title:Revisiting Proprioceptive Sensing for Ar...   \n",
      "22  Title:UOR: Universal Backdoor Attacks on Pre-t...   \n",
      "23  Title:Walking the Walk of AI Ethics: Organizat...   \n",
      "24  Title:Learning from Aggregated Data: Curated B...   \n",
      "\n",
      "                                              Authors  \\\n",
      "0   Ashutosh Dutta, Milan Jain, Arif Khan, Arun Sa...   \n",
      "1                             Jose Alvarez Colmenares   \n",
      "2   Benito van der Zander, Markus Bläser, Maciej L...   \n",
      "3   Hui Yang, Patrick Koopmann, Yue Ma, Nicole Bidoit   \n",
      "4                             Uwe Peters, Mary Carman   \n",
      "5   Ihab F. Ilyas, JP Lacerda, Yunyao Li, Umar Far...   \n",
      "6                     Gyugeun Yoon, Joseph Y. J. Chow   \n",
      "7   Nikolaos Kondylidis, Ilaria Tiddi, Annette ten...   \n",
      "8                         Jiong Yang, Kuldeep S. Meel   \n",
      "9   Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang,...   \n",
      "10                                   Paolo Liberatore   \n",
      "11  Zitang Sun, Yen-Ju Chen, Yung-hao Yang, Shin'y...   \n",
      "12                Michael Cunanan, Michael Thielscher   \n",
      "13               Brendan Conway-Smith, Robert L. West   \n",
      "14  Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi ...   \n",
      "15      Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong   \n",
      "16     Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett   \n",
      "17       Shengchao Hu, Li Shen, Ya Zhang, Dacheng Tao   \n",
      "18                           Junsol Kim, Byungkyu Lee   \n",
      "19  Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sa...   \n",
      "20      Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata   \n",
      "21                       Thomas Lips, Francis wyffels   \n",
      "22  Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Go...   \n",
      "23  Sanna J. Ali, Angèle Christin, Andrew Smart, R...   \n",
      "24  Lin Chen, Thomas Fu, Amin Karbasi, Vahab Mirrokni   \n",
      "\n",
      "                                             Abstract  \\\n",
      "0   Abstract:  Collisions, crashes, and other inci...   \n",
      "1   Abstract:  The field of Artificial Intelligenc...   \n",
      "2   Abstract:  We study formal languages which are...   \n",
      "3   Abstract:  We present a method for extracting ...   \n",
      "4   Abstract:  Many ethical frameworks require art...   \n",
      "5   Abstract:  Applications of large open-domain k...   \n",
      "6   Abstract:  Mobility service route design requi...   \n",
      "7   Abstract:  We propose a method that allows to ...   \n",
      "8   Abstract:  The problem of model counting, also...   \n",
      "9   Abstract:  Instruction tuning for large langua...   \n",
      "10  Abstract:  The three most common representatio...   \n",
      "11  Abstract:  Visual motion processing is essenti...   \n",
      "12  Abstract:  The recent popularity of Wordle has...   \n",
      "13  Abstract:  Attempts to import dual-system desc...   \n",
      "14  Abstract:  Text-guided human motion generation...   \n",
      "15  Abstract:  We study distributionally robust of...   \n",
      "16  Abstract:  Prior work has combined chain-of-th...   \n",
      "17  Abstract:  Prompt-tuning has emerged as a prom...   \n",
      "18  Abstract:  How can we use large language model...   \n",
      "19  Abstract:  Recent artificial intelligence (AI)...   \n",
      "20  Abstract:  Recent semantic segmentation models...   \n",
      "21  Abstract:  Robots that assist humans will need...   \n",
      "22  Abstract:  Backdoors implanted in pre-trained ...   \n",
      "23  Abstract:  Amidst decline in public trust in t...   \n",
      "24  Abstract:  Protecting user privacy is a major ...   \n",
      "\n",
      "                                           Subjects  DownloadUrl  \n",
      "0                   Artificial Intelligence (cs.AI)          NaN  \n",
      "1                   Artificial Intelligence (cs.AI)          NaN  \n",
      "2                   Artificial Intelligence (cs.AI)          NaN  \n",
      "3                   Artificial Intelligence (cs.AI)          NaN  \n",
      "4                   Artificial Intelligence (cs.AI)          NaN  \n",
      "5                   Artificial Intelligence (cs.AI)          NaN  \n",
      "6                   Artificial Intelligence (cs.AI)          NaN  \n",
      "7                   Artificial Intelligence (cs.AI)          NaN  \n",
      "8                   Artificial Intelligence (cs.AI)          NaN  \n",
      "9                   Artificial Intelligence (cs.AI)          NaN  \n",
      "10                  Artificial Intelligence (cs.AI)          NaN  \n",
      "11                  Artificial Intelligence (cs.AI)          NaN  \n",
      "12                  Artificial Intelligence (cs.AI)          NaN  \n",
      "13                  Artificial Intelligence (cs.AI)          NaN  \n",
      "14  Computer Vision and Pattern Recognition (cs.CV)          NaN  \n",
      "15                         Machine Learning (cs.LG)          NaN  \n",
      "16                 Computation and Language (cs.CL)          NaN  \n",
      "17                         Machine Learning (cs.LG)          NaN  \n",
      "18                 Computation and Language (cs.CL)          NaN  \n",
      "19                 Computation and Language (cs.CL)          NaN  \n",
      "20  Computer Vision and Pattern Recognition (cs.CV)          NaN  \n",
      "21                                 Robotics (cs.RO)          NaN  \n",
      "22                 Computation and Language (cs.CL)          NaN  \n",
      "23                    Computers and Society (cs.CY)          NaN  \n",
      "24                         Machine Learning (cs.LG)          NaN  \n"
     ]
    }
   ],
   "source": [
    "#In ra bảng csv vừa tạo được \n",
    "df=pd.read_csv('paper_info.csv')\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"5.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "    # Đọc nội dung tập tin\n",
    "    readfile = file.read()\n",
    "    readfile = readfile.strip()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ngày 21 tháng 02 năm 2023',\n",
       " 'ngày 15 tháng 11 năm 2010',\n",
       " 'ngày 25 tháng 11 năm 2019',\n",
       " 'ngày 14 tháng 12 năm 2004',\n",
       " 'ngày 19 tháng 02 năm 2013',\n",
       " 'ngày 14 tháng 12 năm 2004',\n",
       " 'ngày 16 tháng 01 năm 2023',\n",
       " 'ngày 25 tháng 9 năm 2020',\n",
       " 'ngày 14 tháng 12 năm 2004',\n",
       " 'ngày 25 tháng 5 năm 2007',\n",
       " 'ngày 02 tháng 6 năm 1993',\n",
       " 'ngày 03 tháng 11 năm 2004',\n",
       " 'ngày 10 tháng 8 năm 2005',\n",
       " 'ngày 10 tháng 4 năm 2023']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timelist = re.findall(r\"ngày \\d{1,2} tháng \\d{1,2} năm \\d{4}\",readfile)\n",
    "print(timelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = re.findall(r\"Điều \\d+\\..*?(?=Điều \\d+\\.|\\Z)\", readfile, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điều 1. Phạm vi điều chỉnh và đối tượng áp dụng\n",
      "1. Thông tư này quy định mã số, tiêu chuẩn chức danh nghề nghiệp và xếp lương viên chức chuyên ngành tuyên truyền viên văn hóa.\n",
      "2. Thông tư này áp dụng đối với viên chức tuyên truyền viên văn hóa làm việc trong các đơn vị sự nghiệp công lập và các tổ chức, cá nhân có liên quan.\n",
      "\n",
      "\n",
      "Điều 2. Mã số các chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa\n",
      "1. Tuyên truyền viên văn hóa chính Mã số: V.10.10.34\n",
      "2. Tuyên truyền viên văn hóa Mã số: V.10.10.35\n",
      "3. Tuyên truyền viên văn hóa trung cấp Mã số: V.10.10.36\n",
      "\n",
      "\n",
      "Điều 3. Tiêu chuẩn về đạo đức nghề nghiệp của viên chức chuyên ngành tuyên truyền viên văn hóa\n",
      "1. Có trách nhiệm với công việc được giao, tuân thủ các quy định của pháp luật; thực hiện đúng và đầy đủ nghĩa vụ của viên chức trong hoạt động nghề nghiệp.\n",
      "2. Tâm huyết với nghề, trung thực, khách quan, thẳng thắn; làm việc khoa học, có chính kiến rõ ràng; có thái độ khiêm tốn, đúng mực khi tiếp xúc với nhân dân; có ý thức đấu tranh với những hành vi sai trái, tiêu cực; thực hành tiết kiệm, chống lãng phí.\n",
      "3. Có tinh thần đoàn kết, tích cực, chủ động phối hợp với đồng nghiệp thực hiện nhiệm vụ được giao.\n",
      "4. Không ngừng học tập, rèn luyện nâng cao phẩm chất, trình độ, năng lực.\n",
      "Chương II\n",
      "TIÊU CHUẨN CHỨC DANH NGHỀ NGHIỆP\n",
      "\n",
      "\n",
      "Điều 4. Tuyên truyền viên văn hóa chính - Mã số: V.10.10.34\n",
      "1. Nhiệm vụ:\n",
      "a) Chủ trì xây dựng kế hoạch hoạt động dài hạn, trung hạn, hàng năm về hoạt động tuyên truyền của đơn vị và tổ chức thực hiện sau khi được phê duyệt;\n",
      "b) Chủ trì biên soạn tài liệu và trực tiếp hướng dẫn nghiệp vụ hoạt động tuyên truyền cho cơ sở;\n",
      "c) Chủ trì tổ chức và thực hiện biên tập nội dung chương trình tuyên truyền phục vụ nhiệm vụ chính trị, kinh tế, văn hóa, xã hội; các phong trào, các cuộc vận động lớn; các ngày lễ, kỉ niệm lớn của địa phương và đất nước;\n",
      "d) Tổ chức biên soạn, biên tập các thể loại tin tức, tài liệu tuyên truyền; sáng tác, dàn dựng chương trình, tiết mục văn nghệ, tuyên truyền lưu động; thiết kế, dàn dựng triển lãm, cổ động trực quan;\n",
      "đ) Tham gia các hoạt động đào tạo, bồi dưỡng, hướng dẫn chuyên môn, nghiệp vụ hoạt động tuyên truyền.\n",
      "2. Tiêu chuẩn về trình độ đào tạo, bồi dưỡng:\n",
      "a) Có bằng tốt nghiệp đại học trở lên phù hợp với chuyên ngành tuyên truyền viên văn hóa;\n",
      "b) Có chứng chỉ bồi dưỡng tiêu chuẩn chức danh nghề nghiệp tuyên truyền viên văn hóa.\n",
      "3. Tiêu chuẩn về năng lực chuyên môn, nghiệp vụ:\n",
      "a) Nắm vững chủ trương, đường lối của Đảng, chính sách, pháp luật của Nhà nước về hoạt động tuyên truyền;\n",
      "b) Nắm vững phương pháp tổ chức, hình thức hoạt động tuyên truyền;\n",
      "c) Nắm vững lịch sử, văn hóa, xã hội trên địa bàn được phân công quản lý;\n",
      "d) Có chuyên môn về lĩnh vực văn hóa, nghệ thuật; am hiểu kiến thức, kỹ năng nghiệp vụ tuyên truyền;\n",
      "đ) Có năng lực thu thập, phân tích, tổng hợp và đề xuất các giải pháp để nâng cao chất lượng công tác tuyên truyền ;\n",
      "e) Có kỹ năng sử dụng công nghệ thông tin cơ bản, sử dụng được ngoại ngữ hoặc sử dụng được tiếng dân tộc thiểu số đối với viên chức công tác ở vùng dân tộc thiểu số theo yêu cầu vị trí việc làm.\n",
      "4. Yêu cầu đối với viên chức dự thi hoặc xét thăng hạng chức danh nghề nghiệp tuyên truyền viên văn hóa chính:\n",
      "Có thời gian công tác giữ chức danh nghề nghiệp tuyên truyền viên văn hóa hoặc tương đương từ đủ 09 năm trở lên (không kể thời gian tập sự, thử việc). Trường hợp có thời gian tương đương thì phải có ít nhất 01 năm (đủ 12 tháng) đang giữ chức danh nghề nghiệp tuyên truyền viên văn hóa tính đến ngày hết thời hạn nộp hồ sơ đăng ký dự thi hoặc xét thăng hạng.\n",
      "\n",
      "\n",
      "Điều 5. Tuyên truyền viên văn hóa - Mã số: V.10.10.35\n",
      "1. Nhiệm vụ:\n",
      "a) Xây dựng kế hoạch hàng năm về công tác tuyên truyền được giao và tổ chức thực hiện sau khi được phê duyệt;\n",
      "b) Trực tiếp biên soạn, thiết kế, trình bày các thể loại tin tức, tài liệu tuyên truyền; thực hiện tuyên truyền bằng tin tức, lời nói trực tiếp, thuyết minh, thuyết trình theo đề cương biên tập đã được duyệt;\n",
      "c) Tham gia tổ chức, dàn dựng chương trình, tiết mục văn nghệ, tuyên truyền lưu động; trực tiếp biểu diễn các tiết mục văn nghệ phục vụ nhiệm vụ chính trị;\n",
      "d) Tham gia thiết kế, dàn dựng các triển lãm, cổ động trực quan tại chỗ và lưu động; trực tiếp xây dựng đề cương tổng quát, đề cương chi tiết, biên tập, chú thích hình ảnh theo chủ đề, viết bài, thuyết minh nội dung triển lãm tại chỗ và lưu động.\n",
      "2. Tiêu chuẩn về trình độ đào tạo, bồi dưỡng:\n",
      "a) Có bằng tốt nghiệp đại học trở lên phù hợp với chuyên ngành tuyên truyền viên văn hóa;\n",
      "b) Có chứng chỉ bồi dưỡng tiêu chuẩn chức danh nghề nghiệp tuyên truyền viên văn hóa.\n",
      "3. Tiêu chuẩn về năng lực chuyên môn, nghiệp vụ:\n",
      "a) Nắm vững chủ trương, đường lối của Đảng, chính sách, pháp luật của Nhà nước về hoạt động tuyên truyền;\n",
      "b) Nắm được phương pháp tổ chức, hình thức hoạt động tuyên truyền;\n",
      "c) Có hiểu biết về lịch sử, văn hóa, xã hội trên địa bàn được phân công quản lý;\n",
      "d) Có kiến thức về lĩnh vực văn hóa, nghệ thuật; có một trong các kỹ năng: Thuyết minh, thuyết trình, biểu diễn nhạc cụ, biểu diễn văn nghệ, hội họa, thiết kế, thi công cổ động trực quan hoặc các kỹ năng nghiệp vụ khác phù hợp với hình thức tuyên truyền;\n",
      "đ) Sử dụng thành thạo các phương tiện, thiết bị kỹ thuật phục vụ yêu cầu nhiệm vụ;\n",
      "e) Có kỹ năng sử dụng công nghệ thông tin cơ bản, sử dụng được ngoại ngữ hoặc sử dụng được tiếng dân tộc thiểu số đối với viên chức công tác ở vùng dân tộc thiểu số theo yêu cầu vị trí việc làm.\n",
      "4. Yêu cầu đối với viên chức dự thi hoặc xét thăng hạng chức danh nghề nghiệp tuyên truyền viên văn hóa:\n",
      "Có thời gian công tác giữ chức danh nghề nghiệp tuyên truyền viên văn hoá trung cấp hoặc tương đương từ đủ 03 năm trở lên (không kể thời gian tập sự, thử việc). Trường hợp có thời gian tương đương thì phải có ít nhất 01 năm (đủ 12 tháng) đang giữ chức danh tuyên truyền viên văn hoá trung cấp tính đến ngày hết hạn nộp hồ sơ đăng ký dự thi hoặc xét thăng hạng.\n",
      "\n",
      "\n",
      "Điều 6. Tuyên truyền viên văn hóa trung cấp - Mã số: V.10.10.36\n",
      "1. Nhiệm vụ:\n",
      "a) Trực tiếp tham gia thực hiện tổ chức, biểu diễn các hoạt động tuyên truyền lưu động, hoạt động văn hóa, văn nghệ và các hình thức tuyên truyền cổ động khác tại chỗ hoặc lưu động phù hợp với nhiệm vụ được giao;\n",
      "b) Chụp ảnh, quay phim, thực hiện audio làm tư liệu phục vụ nội dung tuyên truyền.\n",
      "2. Tiêu chuẩn về trình độ đào tạo, bồi dưỡng:\n",
      "a) Có bằng tốt nghiệp trung cấp trở lên phù hợp với chuyên ngành tuyên truyền viên văn hóa;\n",
      "b) Có chứng chỉ bồi dưỡng tiêu chuẩn chức danh nghề nghiệp tuyên truyền viên văn hóa.\n",
      "3. Tiêu chuẩn về năng lực chuyên môn, nghiệp vụ:\n",
      "a) Nắm được chủ trương, đường lối của Đảng, chính sách, pháp luật của Nhà nước về hoạt động tuyên truyền;\n",
      "b) Có kiến thức cơ bản về công tác tuyên truyền và các bộ môn văn hóa nghệ thuật liên quan; có một trong các kỹ năng: Thuyết minh, thuyết trình, biểu diễn nhạc cụ, biểu diễn văn nghệ, hội họa, thiết kế, thi công cổ động trực quan hoặc các kỹ năng nghiệp vụ khác phù hợp với hình thức tuyên truyền;\n",
      "c) Sử dụng được các phương tiện, thiết bị kỹ thuật phục vụ yêu cầu nhiệm vụ;\n",
      "d) Có khả năng ứng dụng công nghệ thông tin cơ bản để thực hiện nhiệm vụ được giao.\n",
      "Chương III\n",
      "XẾP LƯƠNG CHỨC DANH NGHỀ NGHIỆP VIÊN CHỨC CHUYÊN NGÀNH TUYÊN TRUYỀN VIÊN VĂN HÓA\n",
      "\n",
      "\n",
      "Điều 7. Nguyên tắc xếp lương chức danh nghề nghiệp đối với viên chức chuyên ngành tuyên truyền viên văn hóa\n",
      "1. Việc bổ nhiệm và xếp lương vào chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa quy định tại Thông tư này phải căn cứ vào vị trí việc làm, chức trách, nhiệm vụ và chuyên môn, nghiệp vụ đang đảm nhận của viên chức.\n",
      "2. Khi bổ nhiệm và xếp lương vào các chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa tương ứng không được kết hợp nâng bậc lương hoặc thăng hạng chức danh nghề nghiệp viên chức.\n",
      "\n",
      "\n",
      "Điều 8. Cách xếp lương\n",
      "1. Các chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa quy định tại Thông tư này được áp dụng Bảng 3 (Bảng lương chuyên môn, nghiệp vụ đối với cán bộ, viên chức trong các đơn vị sự nghiệp của Nhà nước) ban hành kèm theo Nghị định số 204/2004/NĐ-CP ngày 14 tháng 12 năm 2004 của Chính phủ về chế độ tiền lương đối với cán bộ, công chức, viên chức và lực lượng vũ trang (sau đây viết tắt là Nghị định số 204/2004/NĐ-CP), cụ thể như sau:\n",
      "a) Chức danh nghề nghiệp tuyên truyền viên văn hóa chính được áp dụng ngạch lương của viên chức loại A2, nhóm 2 (A2.2), từ hệ số lương 4,00 đến hệ số lương 6,38;\n",
      "b) Chức danh nghề nghiệp tuyên truyền viên văn hóa được áp dụng ngạch lương của viên chức loại A1, từ hệ số lương 2,34 đến hệ số lương 4,98;\n",
      "c) Chức danh nghề nghiệp tuyên truyền viên văn hóa trung cấp được áp dụng ngạch lương của viên chức loại B, từ hệ số lương 1,86 đến hệ số lương 4,06.\n",
      "2. Sau khi hết thời gian tập sự theo quy định và được cấp có thẩm quyền quản lý viên chức quyết định bổ nhiệm chức danh nghề nghiệp viên chức tuyên truyền viên văn hóa thì thực hiện xếp bậc lương theo chức danh nghề nghiệp được bổ nhiệm như sau:\n",
      "a) Trường hợp bổ nhiệm chức danh nghề nghiệp tuyên truyền viên văn hóa trung cấp:\n",
      "Viên chức có trình độ đào tạo trung cấp khi tuyển dụng, phù hợp với vị trí việc làm tuyển dụng được xếp bậc 1, hệ số lương 1,86, ngạch viên chức loại B;\n",
      "Viên chức có trình độ đào tạo cao đẳng trở lên khi tuyển dụng, phù hợp với vị trí việc làm tuyển dụng được xếp bậc 2, hệ số lương 2,06, ngạch viên chức loại B.\n",
      "b) Trường hợp bổ nhiệm chức danh nghề nghiệp tuyên truyền viên văn hóa:\n",
      "Viên chức có trình độ đào tạo đại học khi tuyển dụng, phù hợp với vị trí việc làm tuyển dụng được xếp bậc 1, hệ số lương 2,34, ngạch viên chức loại A1;\n",
      "Viên chức có trình độ đào tạo thạc sỹ khi tuyển dụng, phù hợp với vị trí việc làm tuyển dụng được xếp bậc 2, hệ số lương 2,67, ngạch viên chức loại A1;\n",
      "Viên chức có trình độ đào tạo tiến sỹ khi tuyển dụng, phù hợp với vị trí việc làm tuyển dụng được xếp bậc 3, hệ số lương 3,00, ngạch viên chức loại A1.\n",
      "3. Việc chuyển xếp lương đối với viên chức từ chức danh nghề nghiệp hiện giữ sang chức danh nghề nghiệp tuyên truyền viên văn hóa quy định tại Thông tư này thực hiện theo hướng dẫn tại Mục II Thông tư số 02/2007/TT- BNV ngày 25 tháng 5 năm 2007 của Bộ trưởng Bộ Nội vụ hướng dẫn xếp lương khi nâng ngạch, chuyển ngạch, chuyển loại công chức, viên chức (sau đây viết tắt là Thông tư số 02/2007/TT-BNV).\n",
      "Chương IV\n",
      "ĐIỀU KHOẢN THI HÀNH\n",
      "\n",
      "\n",
      "Điều 9. Quy định chuyển tiếp\n",
      "1. Viên chức đã được bổ nhiệm vào các ngạch viên chức tuyên truyền viên theo quy định tại Quyết định số 428/TCCP-VC ngày 02 tháng 6 năm 1993 của Bộ trưởng - Trưởng ban Ban Tổ chức - Cán bộ Chính phủ (nay là Bộ trưởng Bộ Nội vụ) ban hành tiêu chuẩn nghiệp vụ ngạch công chức ngành Văn hóa - Thông tin (sau đây viết tắt là Quyết định số 428/TCCP-VC), Quyết định số 78/2004/QĐ-BNV ngày 03 tháng 11 năm 2004 của Bộ trưởng Bộ Nội vụ ban hành danh mục các ngạch công chức và ngạch viên chức (sau đây viết tắt là Quyết định số 78/2004/QĐ-BNV), Thông tư số 80/2005/TT-BNV ngày 10 tháng 8 năm 2005 của Bộ trưởng Bộ Nội vụ hướng dẫn thực hiện chuyển xếp lương đối với cán bộ, công chức, viên chức có trình độ cao đẳng phù hợp với chuyên môn đang làm được bổ nhiệm vào chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa quy định tại Thông tư này, như sau:\n",
      "a) Bổ nhiệm vào chức danh nghề nghiệp tuyên truyền viên văn hóa (mã số V.10.10.35) đối với viên chức hiện đang giữ ngạch tuyên truyền viên chính (mã số 17.177).\n",
      "b) Bổ nhiệm vào chức danh nghề nghiệp tuyên truyền viên văn hóa trung cấp (mã số V.10.10.36) đối với viên chức hiện đang giữ ngạch tuyên truyền viên cao đẳng (mã số 17a.177) hoặc tuyên truyền viên (mã số 17.178).\n",
      "2. Việc chuyển xếp lương vào chức danh nghề nghiệp viên chức quy định tại khoản 1 Điều này đối với viên chức đã được xếp lương vào các ngạch viên chức tuyên truyền viên theo quy định tại Quyết định số 428/TCCP-VC, Quyết định số 78/2004/QĐ-BNV và Nghị định số 204/2004/NĐ-CP được thực hiện theo hướng dẫn tại Thông tư số 02/2007/TT-BNV như sau:\n",
      "a) Trường hợp viên chức có hệ số bậc lương bằng ở ngạch cũ thì thực hiện xếp ngang bậc lương và % phụ cấp thâm niên vượt khung (nếu có) đang hưởng ở ngạch cũ (kể cả tính thời gian xét nâng bậc lương lần sau hoặc xét hưởng phụ cấp thâm niên vượt khung nếu có ở ngạch cũ) vào chức danh nghề nghiệp mới được bổ nhiệm.\n",
      "b) Trường hợp viên chức có trình độ cao đẳng khi tuyển dụng đã được xếp lương viên chức loại A0 theo quy định tại Nghị định số 204/2004/NĐ -CP nay được bổ nhiệm vào chức danh nghề nghiệp tuyên truyền viên văn hóa trung cấp thì việc xếp bậc lương căn cứ vào thời gian công tác có đóng bảo hiểm xã hội bắt buộc theo thang lương, bảng lương do Nhà nước quy định (trừ thời gian tập sự) như sau:\n",
      "Tính từ bậc 2 bảng lương viên chức loại B, cứ sau thời gian 02 năm (đủ 24 tháng) được xếp lên 01 bậc lương (nếu có thời gian đứt quãng mà chưa hưởng chế độ bảo hiểm xã hội thì được cộng dồn). Trường hợp trong thời gian công tác có năm không hoàn thành nhiệm vụ được giao hoặc bị kỷ luật thì bị kéo dài thêm theo chế độ nâng bậc lương thường xuyên. Trường hợp trong thời gian công tác được nâng bậc lương trước thời hạn do lập thành tích xuất sắc trong thực hiện nhiệm vụ thì thời gian được nâng bậc lương trước thời hạ n được tính để xếp lên bậc lương cao hơn trước thời hạn tương ứng. Sau khi quy đổi thời gian để xếp vào bậc lương của chức danh nghề nghiệp được bổ nhiệm, nếu có số tháng chưa đủ 24 tháng, thì số tháng này được tính vào thời gian để xét nâng bậc lương lần sau hoặc xét hưởng phụ cấp thâm niên vượt khung (nếu có).\n",
      "Sau khi chuyển xếp lương vào chức danh nghề nghiệp tuyên truyền viên văn hóa trung cấp nêu trên, nếu hệ số lương được xếp ở chức danh nghề nghiệp này cộng với phụ cấp thâm niên vượt khung (nếu có) thấp hơn hệ số lương đã hưởng ở ngạch cũ thì được hưởng hệ số chênh lệch bảo lưu cho bằng hệ số lương (kể cả phụ cấp thâm niên vượt khung, nếu có) đang hưởng ở ngạch cũ. Hệ số chênh lệch bảo lưu này được hưởng trong suốt thời gian viên chức xếp lương ở chức danh nghề nghiệp tuyên truyền viên văn hóa trung cấp. Sau đó, nếu viên chức được thăng hạng chức danh nghề nghiệp thì được cộng hệ số chênh lệch bảo lưu này vào hệ số lương (kể cả phụ cấp thâm niên vượt khung, nếu có) đang hưởng để xếp lương vào chức danh được bổ nhiệm khi thăng hạng chức danh nghề nghiệp và thôi hưởng hệ số chênh lệch bảo lưu kể từ ngày hưởng lương ở chức danh nghề nghiệp mới.\n",
      "3. Viên chức đã được bổ nhiệm vào các ngạch tuyên truyền viên theo quy định của pháp luật từ trước ngày Thông tư này có hiệu lực thi hành thì được xác định là đáp ứng quy định về tiêu chuẩn chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa quy định tại Thông tư này tương ứng với chức danh nghề nghiệp đã được bổ nhiệm.\n",
      "\n",
      "\n",
      "Điều 10. Tổ chức thực hiện\n",
      "1. Thông tư này là căn cứ để thực hiện việc tuyển dụng, sử dụng và quản lý viên chức chuyên ngành tuyên truyền viên văn hóa làm việc trong các đơn vị sự nghiệp công lập.\n",
      "2. Người đứng đầu các đơn vị sự nghiệp công lập trực tiếp quản lý và sử dụng viên chức có trách nhiệm:\n",
      "a) Rà soát vị trí việc làm của đơn vị, lập phương án bổ nhiệm chức danh nghề nghiệp viên chức chuyên ngành tuyên truyền viên văn hóa thuộc thẩm quyền quản lý, trình cấp có thẩm quyền xem xét, quyết định hoặc quyết định theo thẩm quyền phân cấp;\n",
      "b) Quyết định bổ nhiệm chức danh nghề nghiệp chuyên ngành tuyên truyền viên văn hóa trong các đơn vị sự nghiệp công lập theo thẩm quyền hoặc theo phân cấp, ủy quyền sau khi phương án bổ nhiệm chức danh nghề nghiệp được cấp có thẩm quyền phê duyệt.\n",
      "3. Các Bộ, cơ quan ngang Bộ, cơ quan thuộc Chính phủ, Ủy ban nhân dân tỉnh, thành phố trực thuộc trung ương có trách nhiệm:\n",
      "a) Chỉ đạo các đơn vị thuộc phạm vi quản lý thực hiện bổ nhiệm chức danh nghề nghiệp và xếp lương đối với viên chức giữ chức danh nghề nghiệp chuyên ngành tuyên truyền viên văn hóa theo quy định;\n",
      "b) Quyết định hoặc phân cấp, ủy quyền việc bổ nhiệm và xếp lương chức danh nghề nghiệp chuyên ngành tuyên truyền viên văn hóa đối với viên chức thuộc thẩm quyền quản lý; giải quyết theo thẩm quyền những vướng mắc trong quá trình bổ nhiệm chức danh nghề nghiệp và xếp lương.\n",
      "4. Các tổ chức, đơn vị sự nghiệp ngoài công lập có thể áp dụng quy định tại Thông tư này để tuyển dụng, sử dụng và quản lý đội ngũ người làm việc về chuyên ngành tuyên truyền viên văn hóa.\n",
      "\n",
      "\n",
      "Điều 11. Hiệu lực thi hành\n",
      "1. Thông tư này có hiệu lực thi hành kể từ ngày 10 tháng 4 năm 2023.\n",
      "2. Trường hợp các văn bản dẫn chiếu tại Thông tư này được sửa đổi, bổ sung hoặc thay thế thì thực hiện theo các văn bản đã được sửa đổi, bổ sung hoặc thay thế.\n",
      "\n",
      "\n",
      "Điều 12. Trách nhiệm thi hành\n",
      "1. Bộ trưởng, Thủ trưởng cơ quan ngang Bộ, Thủ trưởng cơ quan thuộc Chính phủ, Chủ tịch Ủy ban nhân dân tỉnh, thành phố trực thuộc trung ương và cơ quan, đơn vị, cá nhân có liên quan chịu trách nhiệm thi hành Thông tư này.\n",
      "2. Trong quá trình thực hiện nếu phát sinh vướng mắc, đề nghị cơ quan, đơn vị, cá nhân kịp thời phản ánh về Bộ Văn hóa, Thể thao và Du lịch để nghiên cứu sửa đổi, bổ sung cho phù hợp.\n",
      "\n",
      "\n",
      "Nơi nhận: - Thủ tướng Chính phủ; - Các Phó Thủ tướng Chính phủ; - Văn phòng Trung ương Đảng; - Văn phòng Chủ tịch nước; - Văn phòng Quốc hội; - Văn phòng Chính phủ; - Tòa án nhân dân tối cao; - Viện Kiểm sát nhân dân tối cao; - Các Bộ, cơ quan ngang Bộ, cơ quan thuộc Chính phủ; - HĐND, UBND các tỉnh, thành phố trực thuộc TW; - Ủy ban TW Mặt trận Tổ quốc Việt Nam; - Cơ quan Trung ương của các đoàn thể; - Cục Kiểm tra VBQPPL-Bộ Tư pháp; - Bộ trưởng, các Thứ trưởng Bộ VHTTDL; - Các Tổng Cục, Cục, Vụ, đơn vị thuộc Bộ VHTTDL; - Sở VHTTDL, Sở VHTT, Sở DL; - Công báo; Cổng TTĐT Chính phủ: Cơ sở dữ liệu quốc gia về pháp luật; - Cổng TTĐT Bộ VHTTDL; - Lưu: VT, Vụ TCCB(300).\n",
      "BỘ TRƯỞNG Nguyễn Văn Hùng\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, law in enumerate(laws, start=1):\n",
    "    print(f\"{law}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02/2023/TT-BVHTTDL',\n",
       " '204/2004/NĐ-CP',\n",
       " '17/2013/NĐ-CP',\n",
       " '204/2004/NĐ-CP',\n",
       " '01/2023/NĐ-CP',\n",
       " '115/2020/NĐ-CP',\n",
       " '204/2004/NĐ-CP',\n",
       " '204/2004/NĐ-CP',\n",
       " '02/2007/TT-',\n",
       " '02/2007/TT-BNV',\n",
       " '78/2004/QĐ-BNV',\n",
       " '78/2004/QĐ-BNV',\n",
       " '80/2005/TT-BNV',\n",
       " '78/2004/QĐ-BNV',\n",
       " '204/2004/NĐ-CP',\n",
       " '02/2007/TT-BNV',\n",
       " '204/2004/NĐ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "law_codes= re.findall(r\"\\d+/\\d+/[\\w\\d-]+\",readfile)\n",
    "print(law_codes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
